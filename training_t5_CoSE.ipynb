{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fc3b357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64705ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 7400\n",
      "Example: {'question': 'The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?', 'choices': ['ignore', 'enforce', 'authoritarian', 'yell at', 'avoid'], 'answer': 'A', 'short_explanation': 'Because ``` Because \"ignore\" best shows the sanctions disregarded the school\\'s efforts to change.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "clean_path = \"/Users/damianli/Desktop/1508_project/common-sense-reasoning/data/csqa_full.jsonl\"\n",
    "\n",
    "# Safely load all lines\n",
    "data_raw = []\n",
    "with open(clean_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            data_raw.append(json.loads(line.strip()))\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "print(\"Loaded:\", len(data_raw))\n",
    "print(\"Example:\", data_raw[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8058e15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning done!\n",
      "Valid: 7400\n",
      "Skipped: 0\n",
      "Saved to: /Users/damianli/Desktop/1508_project/common-sense-reasoning/data/csqa_full_clean.jsonl\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "input_file  = \"/Users/damianli/Desktop/1508_project/common-sense-reasoning/data/csqa_full.jsonl\"\n",
    "output_file = \"/Users/damianli/Desktop/1508_project/common-sense-reasoning/data/csqa_full_clean.jsonl\"\n",
    "\n",
    "cleaned = []\n",
    "skipped = 0\n",
    "\n",
    "def clean_text(t):\n",
    "    if t is None:\n",
    "        return \"\"\n",
    "    t = str(t)\n",
    "\n",
    "    # Remove ``` and ```plaintext\n",
    "    t = re.sub(r\"```plaintext\", \"\", t)\n",
    "    t = t.replace(\"```\", \"\")\n",
    "\n",
    "    # Remove weird line breaks\n",
    "    t = t.replace(\"\\n\", \" \")\n",
    "\n",
    "    # Remove repeated \"Because Because\"\n",
    "    t = t.replace(\"Because Because\", \"Because\")\n",
    "\n",
    "    # Normalize double quotes\n",
    "    t = t.replace('\\\\\"', '\"')\n",
    "    t = t.replace('\"', '\\\\\"')\n",
    "\n",
    "    return t.strip()\n",
    "\n",
    "with open(input_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "        except:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        obj[\"question\"] = clean_text(obj.get(\"question\", \"\"))\n",
    "        obj[\"short_explanation\"] = clean_text(obj.get(\"short_explanation\", \"\"))\n",
    "\n",
    "        if isinstance(obj.get(\"choices\", None), list):\n",
    "            obj[\"choices\"] = [clean_text(c) for c in obj[\"choices\"]]\n",
    "\n",
    "        cleaned.append(obj)\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    for x in cleaned:\n",
    "        f.write(json.dumps(x) + \"\\n\")\n",
    "\n",
    "print(\"Cleaning done!\")\n",
    "print(\"Valid:\", len(cleaned))\n",
    "print(\"Skipped:\", skipped)\n",
    "print(\"Saved to:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e031c733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dataset size: 7400\n",
      "\n",
      "=== Example after formatting ===\n",
      "INPUT TEXT:\n",
      " question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
      "choices: A: ignore; B: enforce; C: authoritarian; D: yell at; E: avoid\n",
      "explain your answer:\n",
      "\n",
      "TARGET TEXT:\n",
      " answer: A. Because \"ignore\" best shows the sanctions disregarded the school's efforts to change.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# =============================\n",
    "# 1. Clean explanation function\n",
    "# =============================\n",
    "def clean_explanation(text):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "\n",
    "    text = str(text)\n",
    "\n",
    "    # Remove all backticks\n",
    "    text = text.replace(\"```\", \"\").replace(\"`\", \"\")\n",
    "\n",
    "    # Remove words like \"plaintext\"\n",
    "    text = re.sub(r\"\\bplaintext\\b|\\btext\\b|\\bpython\\b|\\bjson\\b\",\n",
    "                  \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove newlines\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "\n",
    "    # Collapse multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Normalize repeated \"Because\"\n",
    "    text = re.sub(r\"^(Because\\s+)+\", \"Because \", text)\n",
    "\n",
    "    # Ensure ending with period\n",
    "    if not text.endswith(\".\"):\n",
    "        text += \".\"\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# =================================================\n",
    "# 2. Format choices helper (same as before)\n",
    "# =================================================\n",
    "def format_choices(choice_list):\n",
    "    letters = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n",
    "    return \"; \".join(\n",
    "        f\"{letters[i]}: {choice_list[i]}\" for i in range(len(choice_list))\n",
    "    )\n",
    "\n",
    "\n",
    "# =================================================\n",
    "# 3. Build model-ready dataset (NO dataset.map)\n",
    "# =================================================\n",
    "processed_dataset = []\n",
    "\n",
    "for ex in data_raw:\n",
    "    q = ex[\"question\"]\n",
    "    choices = ex[\"choices\"]\n",
    "    ans = ex[\"answer\"]\n",
    "    expl = clean_explanation(ex[\"short_explanation\"])\n",
    "\n",
    "    input_text = (\n",
    "        f\"question: {q}\\n\"\n",
    "        f\"choices: {format_choices(choices)}\\n\"\n",
    "        f\"explain your answer:\"\n",
    "    )\n",
    "\n",
    "    target_text = f\"answer: {ans}. {expl}\"\n",
    "\n",
    "    processed_dataset.append({\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    })\n",
    "\n",
    "\n",
    "print(\"Processed dataset size:\", len(processed_dataset))\n",
    "print(\"\\n=== Example after formatting ===\")\n",
    "print(\"INPUT TEXT:\\n\", processed_dataset[0][\"input_text\"])\n",
    "print(\"\\nTARGET TEXT:\\n\", processed_dataset[0][\"target_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1908d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: t5-large ...\n",
      "Original processed_dataset size: 7400\n",
      "Train size: 5920\n",
      "Val size: 1480\n",
      "Tokenizing train dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/5920 [00:00<?, ? examples/s]/Users/damianli/Desktop/1508_project/venv/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5920/5920 [00:01<00:00, 5351.72 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1480/1480 [00:00<00:00, 5126.89 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tokenization example (raw text) ===\n",
      "INPUT TEXT:\n",
      " question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
      "choices: A: ignore; B: enforce; C: authoritarian; D: yell at; E: avoid\n",
      "explain your answer:\n",
      "\n",
      "TARGET TEXT:\n",
      " answer: A. Because \"ignore\" best shows the sanctions disregarded the school's efforts to change.\n",
      "\n",
      "=== Tokenization example (IDs) ===\n",
      "input_ids[:20]: tensor([  822,    10,    37, 17210,   581,     8,   496,   130,     3,     9,\n",
      "        24584,    53,  6019,     6,    11,    79,  3776,    12,   125,     8])\n",
      "labels[:20]: tensor([ 1525,    10,    71,     5,  2070,    96,  3191,   127,    15,   121,\n",
      "          200,  1267,     8, 17210,  1028, 12327,     8,   496,    31,     7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 3. Tokenize dataset for T5-Large\n",
    "# =========================================\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import T5Tokenizer\n",
    "import re\n",
    "\n",
    "print(\"Loading tokenizer: t5-large ...\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# If processed_dataset is a Python list, wrap into HF Dataset\n",
    "# -----------------------------------------\n",
    "if isinstance(processed_dataset, list):\n",
    "    processed_dataset = Dataset.from_list(processed_dataset)\n",
    "\n",
    "print(\"Original processed_dataset size:\", len(processed_dataset))\n",
    "\n",
    "# -----------------------------------------\n",
    "# Simple text cleaner (VERY SAFE)\n",
    "# -----------------------------------------\n",
    "def clean_text(t: str) -> str:\n",
    "    if t is None:\n",
    "        return \"\"\n",
    "    t = str(t)\n",
    "\n",
    "    # ÂéªÊéâ ``` ‰πãÁ±ªÁöÑ markdown\n",
    "    t = re.sub(r\"`+\", \"\", t)\n",
    "\n",
    "    # ÂéªÊéâ plaintext / python / json Ëøô‰∫õ tag\n",
    "    t = re.sub(r\"\\bplaintext\\b|\\bpython\\b|\\bjson\\b|\\btext\\b\",\n",
    "               \"\", t, flags=re.IGNORECASE)\n",
    "\n",
    "    # Êç¢Ë°åÂèòÁ©∫Ê†º\n",
    "    t = t.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "\n",
    "    # Â§ö‰∏™Á©∫Ê†ºÂêàÂπ∂\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "    # Â§ÑÁêÜ Because Because...\n",
    "    t = re.sub(r\"^(Because\\s+)+\", \"Because \", t, flags=re.IGNORECASE)\n",
    "\n",
    "    return t.strip()\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# 1) Split dataset (80% train / 20% val)\n",
    "# -----------------------------------------\n",
    "total_size  = len(processed_dataset)\n",
    "train_size  = int(0.8 * total_size)\n",
    "val_size    = total_size - train_size\n",
    "\n",
    "train_dataset = processed_dataset.select(range(train_size))\n",
    "val_dataset   = processed_dataset.select(range(train_size, total_size))\n",
    "\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Val size:\", len(val_dataset))\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# 2) Tokenization function (NO batched=True)\n",
    "# -----------------------------------------\n",
    "def tokenize_function(example):\n",
    "    # ÂÖàÂÅöËΩªÈáèÊ∏ÖÊ¥ó\n",
    "    input_text  = clean_text(example[\"input_text\"])\n",
    "    target_text = clean_text(example[\"target_text\"])\n",
    "\n",
    "    # encode input\n",
    "    model_inputs = tokenizer(\n",
    "        input_text,\n",
    "        max_length=384,       # ÊØî 512 Â∞è‰∏ÄÁÇπÔºåÊõ¥Á®≥„ÄÅÊõ¥Âø´\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # encode labels\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            target_text,\n",
    "            max_length=96,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# 3) Apply tokenizer (per-example, batched=False)\n",
    "# -----------------------------------------\n",
    "print(\"Tokenizing train dataset ...\")\n",
    "train_tokenized = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=False,\n",
    "    remove_columns=train_dataset.column_names,  # Âè™‰øùÁïô token Â≠óÊÆµ\n",
    ")\n",
    "\n",
    "print(\"Tokenizing val dataset ...\")\n",
    "val_tokenized = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=False,\n",
    "    remove_columns=val_dataset.column_names,\n",
    ")\n",
    "\n",
    "# -----------------------------------------\n",
    "# 4) Set format for PyTorch\n",
    "# -----------------------------------------\n",
    "cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "train_tokenized.set_format(type=\"torch\", columns=cols)\n",
    "val_tokenized.set_format(type=\"torch\", columns=cols)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 5) Sanity check\n",
    "# -----------------------------------------\n",
    "print(\"\\n=== Tokenization example (raw text) ===\")\n",
    "print(\"INPUT TEXT:\\n\", train_dataset[0][\"input_text\"])\n",
    "print(\"\\nTARGET TEXT:\\n\", train_dataset[0][\"target_text\"])\n",
    "\n",
    "print(\"\\n=== Tokenization example (IDs) ===\")\n",
    "print(\"input_ids[:20]:\", train_tokenized[0][\"input_ids\"][:20])\n",
    "print(\"labels[:20]:\",    train_tokenized[0][\"labels\"][:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "329f0567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loading T5-large ...\n",
      "Applying LoRA ...\n",
      "\n",
      "===== Trainable parameters =====\n",
      "trainable params: 4,718,592 || all params: 742,386,688 || trainable%: 0.6356\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# -----------------------------------------\n",
    "# Device setup for Mac M-series (MPS)\n",
    "# -----------------------------------------\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# =========================================\n",
    "# 1. Load T5-Large (Mac-safe)\n",
    "# =========================================\n",
    "print(\"Loading T5-large ...\")\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    \"t5-large\",\n",
    "    torch_dtype=torch.float16,   # MPS supports fp16 compute\n",
    "    device_map=None              # must NOT use auto on Mac\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 2. LoRA configuration for T5-Large\n",
    "# =========================================\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                     # ‚Üë increase rank (large model deserves more)\n",
    "    lora_alpha=32,            # scaled with r\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q\", \"v\"],    # T5 attention projection names\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "\n",
    "# =========================================\n",
    "# 3. Apply LoRA\n",
    "# =========================================\n",
    "print(\"Applying LoRA ...\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"\\n===== Trainable parameters =====\")\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21633b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Trainer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damianli/Desktop/1508_project/venv/lib/python3.13/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/1110 [00:00<?, ?it/s]/Users/damianli/Desktop/1508_project/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/damianli/Desktop/1508_project/venv/lib/python3.13/site-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:256.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "  5%|‚ñç         | 50/1110 [04:43<1:39:24,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 263.1364, 'grad_norm': 110.57686614990234, 'learning_rate': 9.009009009009009e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|‚ñâ         | 100/1110 [09:25<1:35:18,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 61.1814, 'grad_norm': 7.3308024406433105, 'learning_rate': 0.00018018018018018018, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñé        | 150/1110 [14:08<1:30:36,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.2689, 'grad_norm': 3.7472586631774902, 'learning_rate': 0.0001921921921921922, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|‚ñà‚ñä        | 200/1110 [18:50<1:26:00,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.8834, 'grad_norm': 3.5459792613983154, 'learning_rate': 0.00018218218218218218, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|‚ñà‚ñà‚ñé       | 250/1110 [23:33<1:21:22,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.5514, 'grad_norm': 3.8082776069641113, 'learning_rate': 0.0001721721721721722, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|‚ñà‚ñà‚ñã       | 300/1110 [28:15<1:16:35,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.52, 'grad_norm': 3.7168185710906982, 'learning_rate': 0.00016216216216216218, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 350/1110 [32:59<1:11:23,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.138, 'grad_norm': 3.1321218013763428, 'learning_rate': 0.00015215215215215214, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 370/1110 [38:12<1:09:42,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.405029296875, 'eval_runtime': 200.2393, 'eval_samples_per_second': 7.391, 'eval_steps_per_second': 7.391, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damianli/Desktop/1508_project/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 36%|‚ñà‚ñà‚ñà‚ñå      | 400/1110 [41:03<1:07:02,  5.67s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9368, 'grad_norm': 3.5109355449676514, 'learning_rate': 0.00014214214214214215, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|‚ñà‚ñà‚ñà‚ñà      | 450/1110 [45:47<1:02:13,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9386, 'grad_norm': 3.1115074157714844, 'learning_rate': 0.00013213213213213214, 'epoch': 1.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 500/1110 [50:30<57:42,  5.68s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.813, 'grad_norm': 3.2237462997436523, 'learning_rate': 0.00012212212212212213, 'epoch': 1.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 550/1110 [55:13<52:36,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7097, 'grad_norm': 3.571535587310791, 'learning_rate': 0.00011211211211211213, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 600/1110 [59:56<48:14,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8321, 'grad_norm': 8.747725486755371, 'learning_rate': 0.00010210210210210212, 'epoch': 1.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 650/1110 [1:04:40<43:30,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.6423, 'grad_norm': 3.813053607940674, 'learning_rate': 9.20920920920921e-05, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 700/1110 [1:09:22<38:32,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.6484, 'grad_norm': 3.650789976119995, 'learning_rate': 8.208208208208209e-05, 'epoch': 1.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 740/1110 [1:16:30<34:48,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3818359375, 'eval_runtime': 200.4041, 'eval_samples_per_second': 7.385, 'eval_steps_per_second': 7.385, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damianli/Desktop/1508_project/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 750/1110 [1:17:27<48:48,  8.13s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.5536, 'grad_norm': 3.3807427883148193, 'learning_rate': 7.207207207207208e-05, 'epoch': 2.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 800/1110 [1:22:11<29:24,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.4478, 'grad_norm': 3.6621367931365967, 'learning_rate': 6.206206206206206e-05, 'epoch': 2.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 850/1110 [1:26:55<24:29,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.2706, 'grad_norm': 4.251887798309326, 'learning_rate': 5.2052052052052056e-05, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 900/1110 [1:31:38<19:44,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.436, 'grad_norm': 3.684340238571167, 'learning_rate': 4.204204204204204e-05, 'epoch': 2.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 950/1110 [1:36:21<15:04,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.4946, 'grad_norm': 3.784519910812378, 'learning_rate': 3.203203203203203e-05, 'epoch': 2.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1000/1110 [1:41:04<10:23,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.4106, 'grad_norm': 3.4443304538726807, 'learning_rate': 2.2022022022022024e-05, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1050/1110 [1:45:47<05:41,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.6377, 'grad_norm': 3.250004529953003, 'learning_rate': 1.2012012012012012e-05, 'epoch': 2.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1100/1110 [1:50:31<00:57,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.5211, 'grad_norm': 3.398838996887207, 'learning_rate': 2.002002002002002e-06, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1110/1110 [1:51:28<00:00,  5.67s/it]/Users/damianli/Desktop/1508_project/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                                                     \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1110/1110 [1:54:48<00:00,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.376220703125, 'eval_runtime': 200.1498, 'eval_samples_per_second': 7.394, 'eval_steps_per_second': 7.394, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1110/1110 [1:54:49<00:00,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6889.3988, 'train_samples_per_second': 2.578, 'train_steps_per_second': 0.161, 'train_loss': 20.911044366510065, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1110, training_loss=20.911044366510065, metrics={'train_runtime': 6889.3988, 'train_samples_per_second': 2.578, 'train_steps_per_second': 0.161, 'total_flos': 2.903151023751168e+16, 'train_loss': 20.911044366510065, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "print(\"Preparing Trainer ...\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5_large_csqa_lora\",\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    num_train_epochs=3,\n",
    "\n",
    "    per_device_train_batch_size=1,     \n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=16,    \n",
    "\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.1,\n",
    "\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    dataloader_num_workers=0,\n",
    "    torch_compile=False,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03c10b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model (t5-large) ...\n",
      "Loading LoRA adapter from: /Users/damianli/Desktop/1508_project/common-sense-reasoning/t5_large_csqa_lora/checkpoint-1110\n",
      "Merging LoRA weights ...\n",
      "\n",
      "Merged model saved to: /Users/damianli/Desktop/1508_project/common-sense-reasoning/t5_large_csqa_lora_merged\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "# ============================================\n",
    "# 1) Select the correct latest checkpoint\n",
    "# ============================================\n",
    "adapter_path = (\n",
    "    \"/Users/damianli/Desktop/1508_project/common-sense-reasoning/\"\n",
    "    \"t5_large_csqa_lora/checkpoint-1110\"\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# 2) Load the SAME base model used for training\n",
    "# ============================================\n",
    "print(\"Loading base model (t5-large) ...\")\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(\n",
    "    \"t5-large\",\n",
    "    torch_dtype=\"float16\" if device == \"mps\" else None,\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# 3) Load LoRA adapter into base model\n",
    "# ============================================\n",
    "print(\"Loading LoRA adapter from:\", adapter_path)\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "# ============================================\n",
    "# 4) Merge LoRA weights ‚Üí standalone full model\n",
    "# ============================================\n",
    "print(\"Merging LoRA weights ...\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# ============================================\n",
    "# 5) Save merged full model\n",
    "# ============================================\n",
    "merged_path = (\n",
    "    \"/Users/damianli/Desktop/1508_project/common-sense-reasoning/\"\n",
    "    \"t5_large_csqa_lora_merged\"\n",
    ")\n",
    "model.save_pretrained(merged_path)\n",
    "\n",
    "# ============================================\n",
    "# 6) Save tokenizer\n",
    "# ============================================\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n",
    "tokenizer.save_pretrained(merged_path)\n",
    "\n",
    "print(\"\\nMerged model saved to:\", merged_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bf10708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: B. Because a freezer is designed to store ice for long-term storage.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "model_path = \"/Users/damianli/Desktop/1508_project/common-sense-reasoning/t5_large_csqa_lora_merged\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "\n",
    "input_text = \"\"\"question: Where would you put ice to keep it frozen?\n",
    "choices: A: oven; B: freezer; C: desk; D: backpack; E: pocket\n",
    "explain your answer:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade9ad22",
   "metadata": {},
   "source": [
    "### Step 4: Small Evaluation Suite (Pre- & Post- fine-tuning)\n",
    "\n",
    "**Goal**\n",
    "\n",
    "Before optimizing or switching to larger models (e.g., T5-large), we want a small but clear evaluation protocol to:\n",
    "\n",
    "- Probe the model‚Äôs ability to:\n",
    "  - Select the correct answer choice (A/B/C/‚Ä¶)\n",
    "  - Generate a coherent, on-topic explanation\n",
    "- Record **before/after** performance for reporting.\n",
    "\n",
    "**Evaluation setup**\n",
    "\n",
    "- Sample 10 examples from the cleaned dataset (`csqa_full_clean.jsonl`) with a fixed random seed for reproducibility.\n",
    "- Use the same input format as training:\n",
    "  - `question: ...`\n",
    "  - `choices: A: ...; B: ...; ...`\n",
    "  - `explain your answer:`\n",
    "- Let the model generate:\n",
    "  - A combined answer + explanation text (e.g., `answer: B. Because ...`)\n",
    "\n",
    "**Metrics**\n",
    "\n",
    "For each example we record:\n",
    "\n",
    "1. **Predicted answer letter** (A‚ÄìE)\n",
    "2. **Answer correctness** (match gold answer letter ‚Üí 0/1)\n",
    "3. **Explanation text** (raw string)\n",
    "4. **Explanation length** (number of words)\n",
    "5. **Heuristic quality flags** (e.g., contains ‚Äúbecause‚Äù, non-empty)\n",
    "\n",
    "We will:\n",
    "\n",
    "- Print a human-readable summary for each example.\n",
    "- Compute **overall accuracy** on the 10-question subset.\n",
    "- Save a `.csv` file with all fields so we can compare:\n",
    "  - Baseline model (e.g. `t5-small`)\n",
    "  - Fine-tuned model (e.g. `t5_csqa_lora_merged`)\n",
    "  - Later: T5-large or other variants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfe0f066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loaded 8 baseline questions.\n",
      "\n",
      "======================================================================\n",
      "QUESTION 1\n",
      "Q: Sammy wanted to go to where the people were. Where might he go?\n",
      "Choices: ['race track', 'populated areas', 'the desert', 'apartment', 'roadblock']\n",
      "\n",
      "Model Output:\n",
      " answer: B. Because \"populated areas\" best fits Sammy's desire to visit populated areas.\n",
      "\n",
      "Predicted: B | Gold: B | Correct: True\n",
      "======================================================================\n",
      "QUESTION 2\n",
      "Q: Where do you store fresh vegetables?\n",
      "Choices: ['garage', 'refrigerator', 'bookshelf', 'bathroom', 'attic']\n",
      "\n",
      "Model Output:\n",
      " answer: B. Because refrigerators are ideal for storing fresh vegetables.\n",
      "\n",
      "Predicted: B | Gold: B | Correct: True\n",
      "======================================================================\n",
      "QUESTION 3\n",
      "Q: If you heat water to 100 degrees Celsius, what will happen?\n",
      "Choices: ['it will freeze', 'it will boil', 'it will rust', 'it will glow', 'it will evaporate']\n",
      "\n",
      "Model Output:\n",
      " answer: B. Because boiling occurs when water reaches 100 degrees Celsius.\n",
      "\n",
      "Predicted: B | Gold: B | Correct: True\n",
      "======================================================================\n",
      "QUESTION 4\n",
      "Q: What do people usually use to dry their hands after washing?\n",
      "Choices: ['towel', 'hammer', 'blanket', 'pillow', 'shoe']\n",
      "\n",
      "Model Output:\n",
      " answer: A. Because a towel is commonly used to dry hands after washing.\n",
      "\n",
      "Predicted: A | Gold: A | Correct: True\n",
      "======================================================================\n",
      "QUESTION 5\n",
      "Q: Where would you typically find books to read?\n",
      "Choices: ['library', 'swimming pool', 'factory', 'garage', 'freeway']\n",
      "\n",
      "Model Output:\n",
      " answer: A. Because libraries offer a wide variety of books for reading, making them the best choice.\n",
      "\n",
      "Predicted: A | Gold: A | Correct: True\n",
      "======================================================================\n",
      "QUESTION 6\n",
      "Q: If someone wants to relax and reduce stress, what might they do?\n",
      "Choices: ['meditate', 'argue', 'shout', 'work more', 'run into danger']\n",
      "\n",
      "Model Output:\n",
      " answer: A. Because meditation is the most effective way to relax and reduce stress.\n",
      "\n",
      "Predicted: A | Gold: A | Correct: True\n",
      "======================================================================\n",
      "QUESTION 7\n",
      "Q: What tool is commonly used to tighten screws?\n",
      "Choices: ['screwdriver', 'spoon', 'pencil', 'comb', 'fork']\n",
      "\n",
      "Model Output:\n",
      " answer: A. Because a screwdriver is the most common tool for tightening screws.\n",
      "\n",
      "Predicted: A | Gold: A | Correct: True\n",
      "======================================================================\n",
      "QUESTION 8\n",
      "Q: Where would you likely find many wild animals living together?\n",
      "Choices: ['forest', 'kitchen', 'bathroom', 'rooftop', 'office']\n",
      "\n",
      "Model Output:\n",
      " answer: A. Because forests are ideal habitats for wild animals to live together.\n",
      "\n",
      "Predicted: A | Gold: A | Correct: True\n",
      "\n",
      "======================================================================\n",
      "Final Accuracy: 8/8 = 1.00\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Baseline Evaluation for CSQA Model\n",
    "# =========================================\n",
    "\n",
    "import torch\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 1. Baseline test questions\n",
    "# -----------------------------------------\n",
    "baseline_samples = [\n",
    "    {\n",
    "        \"question\": \"Sammy wanted to go to where the people were. Where might he go?\",\n",
    "        \"choices\": [\"race track\", \"populated areas\", \"the desert\", \"apartment\", \"roadblock\"],\n",
    "        \"answer\": \"B\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Where do you store fresh vegetables?\",\n",
    "        \"choices\": [\"garage\", \"refrigerator\", \"bookshelf\", \"bathroom\", \"attic\"],\n",
    "        \"answer\": \"B\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"If you heat water to 100 degrees Celsius, what will happen?\",\n",
    "        \"choices\": [\"it will freeze\", \"it will boil\", \"it will rust\", \"it will glow\", \"it will evaporate\"],\n",
    "        \"answer\": \"B\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do people usually use to dry their hands after washing?\",\n",
    "        \"choices\": [\"towel\", \"hammer\", \"blanket\", \"pillow\", \"shoe\"],\n",
    "        \"answer\": \"A\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Where would you typically find books to read?\",\n",
    "        \"choices\": [\"library\", \"swimming pool\", \"factory\", \"garage\", \"freeway\"],\n",
    "        \"answer\": \"A\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"If someone wants to relax and reduce stress, what might they do?\",\n",
    "        \"choices\": [\"meditate\", \"argue\", \"shout\", \"work more\", \"run into danger\"],\n",
    "        \"answer\": \"A\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What tool is commonly used to tighten screws?\",\n",
    "        \"choices\": [\"screwdriver\", \"spoon\", \"pencil\", \"comb\", \"fork\"],\n",
    "        \"answer\": \"A\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Where would you likely find many wild animals living together?\",\n",
    "        \"choices\": [\"forest\", \"kitchen\", \"bathroom\", \"rooftop\", \"office\"],\n",
    "        \"answer\": \"A\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(baseline_samples)} baseline questions.\\n\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# 2. Helper to format choices\n",
    "# -----------------------------------------\n",
    "def format_choices_eval(choices):\n",
    "    letters = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "    return \"; \".join([f\"{letters[i]}: {choices[i]}\" for i in range(len(choices))])\n",
    "\n",
    "# -----------------------------------------\n",
    "# 3. Run model\n",
    "# -----------------------------------------\n",
    "def run_model(question, choices):\n",
    "    input_text = (\n",
    "        f\"question: {question}\\n\"\n",
    "        f\"choices: {format_choices_eval(choices)}\\n\"\n",
    "        f\"explain your answer:\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=120,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return decoded\n",
    "\n",
    "# -----------------------------------------\n",
    "# 4. Evaluation loop\n",
    "# -----------------------------------------\n",
    "correct = 0\n",
    "results = []\n",
    "\n",
    "for i, sample in enumerate(baseline_samples):\n",
    "    q = sample[\"question\"]\n",
    "    choices = sample[\"choices\"]\n",
    "    gold = sample[\"answer\"]\n",
    "\n",
    "    output = run_model(q, choices)\n",
    "\n",
    "    # Predict letter by pattern\n",
    "    pred = None\n",
    "    for letter in [\"A\", \"B\", \"C\", \"D\", \"E\"]:\n",
    "        if f\"answer: {letter}\" in output:\n",
    "            pred = letter\n",
    "            break\n",
    "        if output.strip().startswith(letter + \".\"):\n",
    "            pred = letter\n",
    "            break\n",
    "\n",
    "    is_correct = (pred == gold)\n",
    "    correct += int(is_correct)\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"QUESTION {i+1}\")\n",
    "    print(\"Q:\", q)\n",
    "    print(\"Choices:\", choices)\n",
    "    print(\"\\nModel Output:\\n\", output)\n",
    "    print(f\"\\nPredicted: {pred} | Gold: {gold} | Correct: {is_correct}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"Final Accuracy: {correct}/{len(baseline_samples)} = {correct/len(baseline_samples):.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
